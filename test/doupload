#!/bin/zsh

# Performs an upload and retrieval test using the guppy client CLI.
#
# Usage: test/doupload [--database-url <url>]
#
# Options:
#   --database-url <url>  Use PostgreSQL instead of SQLite. The URL can either
#                         include a database name (e.g., postgres://localhost/mydb)
#                         or omit it (e.g., postgres://localhost), in which case a
#                         temporary database is created and dropped on exit.
#
# The test will:
# 1. Create a temporary email inbox using maildrop.cc
# 2. Create a new space
# 3. Upload some random data to the space
# 4. Retrieve the full data
# 5. Retrieve a subdirectory of the data
# 6. Reset the client and log in again
# 7. Retrieve the full data again
# 8. Verify that all retrieved data matches the original

set -e
set -o pipefail

# Parse arguments
database_url=""
while [[ $# -gt 0 ]]; do
	case $1 in
		--database-url)
			database_url="$2"
			shift 2
			;;
		*)
			echo "Unknown option: $1"
			echo "Usage: test/doupload [--database-url <url>]"
			exit 1
			;;
	esac
done

# If a database URL was provided, set up the database.
created_db_name=""
if [[ -n "$database_url" ]]; then
	# Check for psql
	if ! command -v psql &> /dev/null; then
		echo "psql could not be found, please install it to use --database-url."
		exit 1
	fi

	# Parse the URL to check if a database name is included.
	# Split off query string first, then check the path component.
	# postgres://user:pass@host:port/dbname?sslmode=disable -> has db name
	# postgres://user:pass@host:port?sslmode=disable        -> no db name
	# postgres://user:pass@host:port/?sslmode=disable       -> no db name
	# postgres://user:pass@host:port                        -> no db name
	url_base="${database_url%%\?*}"        # everything before ?
	url_query=""
	if [[ "$database_url" == *"?"* ]]; then
		url_query="?${database_url#*\?}"   # ?sslmode=disable&...
	fi

	# After the scheme (postgres://), find the path after the host
	after_scheme="${url_base#*://}"         # user:pass@host:port/dbname
	db_path="${after_scheme#*/}"            # dbname (or same as after_scheme if no /)

	# No db name if: no / found (db_path == after_scheme), or path is empty
	if [[ "$db_path" == "$after_scheme" ]] || [[ -z "$db_path" ]]; then
		# No database name provided; create a temporary one
		created_db_name="guppy_test_$(head -c 8 < /dev/urandom | xxd -p)"
		# Strip trailing slash from base
		url_base="${url_base%/}"
		# Connect to the default 'postgres' database to create ours
		psql "${url_base}/postgres${url_query}" -c "CREATE DATABASE ${created_db_name};" >/dev/null
		database_url="${url_base}/${created_db_name}${url_query}"
		echo "Created temporary database: ${created_db_name}"
	fi
fi

export STORACHA_SERVICE_URL="https://staging.up.warm.storacha.network"
export STORACHA_SERVICE_DID="did:web:staging.up.warm.storacha.network"
export STORACHA_RECEIPTS_URL="https://staging.up.warm.storacha.network/receipt/"
export STORACHA_INDEXING_SERVICE_URL="https://staging.indexer.warm.storacha.network"
export STORACHA_INDEXING_SERVICE_DID="did:web:staging.indexer.warm.storacha.network"

# Check for dependencies
if ! command -v jq &> /dev/null; then
		echo "jq could not be found, please install it to run this script."
		exit 1
fi
if ! command -v htmlq &> /dev/null; then
		echo "htmlq could not be found, please install it to run this script."
		exit 1
fi

# Change to the directory of this script
cd "$(dirname "$0")"

# Teeing to /dev/fd/3 will show output on stdout while still capturing it.
exec 3>&1


sandbox="doupload-dir"

# Function wrapper to properly invoke guppy with word splitting
guppy() {
	local db_args=()
	if [[ -n "$database_url" ]]; then
		db_args+=(--database-url "$database_url")
	fi
	go run .. --data-dir ./$sandbox/storacha "${db_args[@]}" "$@"
}

dataDir="$sandbox/data"
outDir1="$sandbox/out1"
outDir2="$sandbox/out2"
outDir3="$sandbox/out3"

# Track background processes to kill on exit
cleanup_pids=()

# Cleanup function to kill background processes and their children
cleanup() {
	local pid=
	for pid in "${cleanup_pids[@]}"; do
		if [ -n "$pid" ] && kill -0 "$pid" 2>/dev/null; then
			echo "Cleaning up background process $pid and its children"
			# Kill all child processes first
			pkill -P "$pid" 2>/dev/null || true
			# Then kill the parent
			kill "$pid" 2>/dev/null || true
			# Wait a moment, then force kill if still alive
			sleep 0.1
			if kill -0 "$pid" 2>/dev/null; then
				pkill -9 -P "$pid" 2>/dev/null || true
				kill -9 "$pid" 2>/dev/null || true
			fi
		fi
	done

	# Drop temporary database if we created one
	if [[ -n "$created_db_name" ]]; then
		echo "Dropping temporary database: ${created_db_name}"
		local drop_url_base="${database_url%%\?*}"
		drop_url_base="${drop_url_base%/${created_db_name}}"
		local drop_url_query=""
		if [[ "$database_url" == *"?"* ]]; then
			drop_url_query="?${database_url#*\?}"
		fi
		psql "${drop_url_base}/postgres${drop_url_query}" -c "DROP DATABASE IF EXISTS ${created_db_name};" >/dev/null 2>&1 || true
		
	fi
}

# Set up trap to cleanup on EXIT, INT, TERM
trap cleanup EXIT INT TERM

# Track the last command for error reporting
last_command=""
trap 'last_command=$ZSH_DEBUG_CMD' DEBUG

# Error handler
handle_error() {
	local exit_code=$1
	local failed_command=$2
	echo
	echo "‚ùå Command failed with exit code $exit_code: $failed_command"
}

# Handle errors with some printed output
trap 'handle_error $? "$last_command"' ERR

main () {
	rm -rf "$sandbox"
	mkdir -p "$sandbox"

	go run github.com/storacha/randdir@latest --output "$dataDir/large-files" --size 50MB --min-file-size 10MB
	go run github.com/storacha/randdir@latest --output "$dataDir/small-files" --size 5MB --min-file-size 100KB --max-file-size 1MB

	# Generate a random maildrop email address
	local random_id=$(head -c 10 < /dev/urandom | base32 | tr "[:upper:]" "[:lower:]")
	local account="${random_id}@maildrop.cc"
	echo "üîê Logging in as $account"

	# Log in
	log_in "$account"

	echo
	echo "üéÅ Generating new space"
	space=$(guppy space generate | tee /dev/fd/3)

	echo
	echo "üìú Listing space info"
	guppy space info "$space"

	echo
	echo "üì§ Uploading data from $dataDir to space $space"
	guppy upload source add "$space" "$dataDir"
	rootCID=$(guppy upload "$space" | tee /dev/fd/3 | grep 'Upload completed successfully:' | awk '{print $4}')

	echo
	echo "üì• Retrieving data from space $space with root CID $rootCID to $outDir1"
	guppy retrieve "$space" "$rootCID" "$outDir1"

	echo
	echo "üì• Retrieving data from only subdir with root CID $rootCID to $outDir2"
	guppy retrieve "$space" "$rootCID/small-files" "$outDir2"


	echo
	echo "üîÑ Resetting client"
	guppy reset
	echo "üîê Logging in as $account again"

	# Log in again
	log_in "$account"

	# Remove from cleanup list once completed
	cleanup_pids=("${cleanup_pids[@]/$login_pid}")

	echo
	echo "üì• Retrieving data from space $space with root CID $rootCID to $outDir3"
	guppy retrieve "$space" "$rootCID" "$outDir3"

	echo "‚ÜîÔ∏è Verifying retrieved data matches original"
	diff -r "$dataDir" "$outDir1"
	diff -r "$dataDir/small-files" "$outDir2"
	diff -r "$dataDir" "$outDir3"
	echo "‚úÖ Data verified!"

	jq -n \
		--arg account "$account" \
		--arg space "$space" \
		--arg rootCID "$rootCID" \
		--arg dataDir "$dataDir" \
		--arg subdir "subdir" \
		'{$account, $space, $rootCID, $dataDir, $subdir}' > "$sandbox/test-params.json"
}

log_in() {
	local account="$1"
	# Start login in background
	guppy login "$account" >/dev/null &
	login_pid=$!
	cleanup_pids+=("$login_pid")

	# Verify email
	verify_email "$account"

	# Wait for login to complete
	wait $login_pid

	# Remove from cleanup list once completed
	cleanup_pids=("${cleanup_pids[@]/$login_pid}")
}

seen_message_ids=()

# Function to check maildrop inbox and click verification link
verify_email() {
	local email="$1"
	local inbox_name="${email%%@*}"

	echo "‚è≥ Waiting for verification email..."
	local max_attempts=30
	local attempt=0

	while [ $attempt -lt $max_attempts ]; do
		# Fetch inbox from Maildrop using GraphQL API
		local inbox_query='{"query":"query { inbox(mailbox:\"'${inbox_name}'\") { id subject } }"}'
		local inbox_response=$(curl -s -X POST \
			-H 'content-type: application/json' \
			--url https://api.maildrop.cc/graphql \
			--data "$inbox_query")

		# Find the first message that we haven't seen yet
		local message_id=""
		local all_message_ids=($(jq -r '.data.inbox[].id // empty' <<< "$inbox_response"))

		for id in "${all_message_ids[@]}"; do
			# Check if this ID is in the seen list
			if [[ ! " ${seen_message_ids[@]} " =~ " ${id} " ]]; then
				message_id="$id"
				seen_message_ids+=("$message_id")
				break
			fi
		done

		if [ -n "$message_id" ]; then
			echo "üì¨ Found email (ID: $message_id), retrieving verification link..."

			# Fetch the message content (HTML) using GraphQL
			local message_query='{"query":"query { message(mailbox:\"'${inbox_name}'\", id:\"'${message_id}'\") { html } }"}'
			local message_response=$(curl -s -X POST \
				-H 'content-type: application/json' \
				--url https://api.maildrop.cc/graphql \
				--data "$message_query")

			# Extract the HTML content from the GraphQL response and decode it
			local message_html=$(jq -r '.data.message.html' <<< "$message_response")

			# Extract all https links from the HTML
			local verify_url=$(htmlq --attribute href a.button <<< "$message_html")

			echo "‚úÖ Submitting approval form to: $verify_url"
			curl -sL -X POST "$verify_url" > /dev/null

			echo "‚úì Email verified!"
			return 0
		else
			echo "‚ö†Ô∏è No verification link found in email, retrying..."
		fi

		attempt=$((attempt + 1))
		sleep 2
	done

	echo "‚ùå Failed to receive verification email after ${max_attempts} attempts"
	return 1
}

main